{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629d26c3",
   "metadata": {},
   "source": [
    "![Business Description Clustering using NLP](../images/image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f82724",
   "metadata": {},
   "source": [
    "# Business Description Clustering with SBERT\n",
    "\n",
    "This project builds a natural language processing pipeline to cluster **S&P 500 companies** based on the semantic content of their business descriptions using **SBERT embeddings** and **hierarchical clustering**.\n",
    "\n",
    "The primary objective is to uncover functionally similar companies by analyzing their descriptions and grouping them according to operational or sector-based characteristics.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Extract and preprocess business descriptions for S&P 500 companies\n",
    "- Generate high-quality text embeddings using SBERT\n",
    "- Apply dimensionality reduction techniques to streamline embeddings\n",
    "- Perform hierarchical clustering to identify natural groupings and sectoral patterns\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#1-setup)  \n",
    "2. [Data Collection](#2-data-collection)\n",
    "3. [Text Preprocessing](#3-text-preprocessing)\n",
    "4. [SBERT Embedding Generation](#4-sbert-embedding-generation)\n",
    "5. [Dimension Reduction](#5-dimension-reduction)\n",
    "6. [Hierarchical Clustering](#6-hierarchical-clustering)    \n",
    "7. [Conclusion](#7-conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5aa30",
   "metadata": {},
   "source": [
    "## 1. Setup \n",
    "\n",
    "In this section, I import all necessary libraries for data collection, processing, and analysis. I use `yfinance` for gathering company data, `nltk` and `spacy` for text processing, `sentence_transformers` for generating embeddings, and various scikit-learn components for dimensionality reduction and clustering.\n",
    "\n",
    "The SBERT model \"multi-qa-MiniLM-L6-cos-v1\" is selected for its efficiency and effectiveness with business text data while maintaining a reasonable computational footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP tools\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "# Embedding and clustering tools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ignore warnings for a cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize the SBERT model (using a compact model optimized for semantic similarity)\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# Download the small English language model for spaCy \n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9d496",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "This section focuses on gathering data for our analysis. We use two main sources:\n",
    "\n",
    "1. **Wikipedia**: To extract the current list of S&P 500 companies and their industry classifications\n",
    "2. **Yahoo Finance API (via yfinance)**: To obtain detailed business descriptions for each company\n",
    "\n",
    "The S&P 500 list includes company tickers, names, sectors, and sub-industry classifications according to the Global Industry Classification Standard (GICS). This structured data will help us validate our clustering results later.\n",
    "\n",
    "After retrieval, I perform basic data cleaning on the ticker symbols to ensure compatibility with the Yahoo Finance API format (replacing periods with dashes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77183fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract S&P 500 tickers from Wikipedia\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "sp500_table = pd.read_html(url)[0]  # The first table is the S&P 500 list\n",
    "tickers = sp500_table['Symbol'].tolist()\n",
    "\n",
    "# Replace periods with dashes (e.g., BRK.B -> BRK-B) as yfinance requires this format\n",
    "tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
    "\n",
    "# Use yfinance to extract business descriptions for each ticker\n",
    "bd_data = []\n",
    "errors = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        description = info.get('longBusinessSummary', '') # Retrieve the long business summary\n",
    "        bd_data.append({'Symbol': ticker, 'long_bd': description})\n",
    "    except Exception as e:\n",
    "        errors.append({'Symbol': ticker, 'error': str(e)})\n",
    "\n",
    "# Add business description data to S&P 500 table\n",
    "df = pd.merge(sp500_table, pd.DataFrame(bd_data), on='Symbol', how='left')\n",
    "\n",
    "# Print number of errors\n",
    "print(f\"Number of errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d89a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names for consistency\n",
    "df.columns = df.columns.str.lower()\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "df.columns = df.columns.str.replace(\".\", \"_\")\n",
    "df.columns = df.columns.str.replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a574af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus only on relevant columns for our analysis\n",
    "df = df.drop(columns=['headquarters_location', 'date_added', 'cik', 'founded'])\n",
    "\n",
    "# Rename columns for clarity and consistency\n",
    "df.rename(columns={'symbol': 'ticker', 'security': 'company', 'gics_sector': 'industry', 'gics_sub_industry': 'sub_industry', 'long_bd': 'long_bd'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first five rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Industries and Sub-Industries in the S&P 500\n",
    "print(f\"Number of Industries: {len(df['industry'].value_counts())}\")\n",
    "print(f\"Number of Sub-Industries: {len(df['sub_industry'].value_counts())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa8e38",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "SBERT works well with raw text, so only light preprocessing is needed to clean and focus the business descriptions. The goal is to remove noise and keep only the most relevant content. Here's what we do:\n",
    "\n",
    "- **Remove any rows with missing descriptions** to avoid issues during embedding.\n",
    "- **Strip out HTML tags** like `<br>` or `<div>` to keep only readable text.\n",
    "- **Clean up extra spaces, tabs, or line breaks** and trim the text for consistency.\n",
    "- **Keep only the first two sentences of the business descriptions.**\n",
    "\n",
    "This focused approach helps to capture the core business activities while reducing noise from promotional or peripheral content that often appears later in business descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddecf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text and retains only the first two sentences describing a business activity.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    if len(sentences) >= 2:\n",
    "        return sentences[0] + \" \" + sentences[1]  \n",
    "    return None\n",
    "\n",
    "# Apply preprocessing to the DataFrame\n",
    "df['clean_bd'] = df['long_bd'].apply(preprocess_text)\n",
    "df = df.dropna(subset=['clean_bd'])\n",
    "\n",
    "print(\"Preprocessing complete. Example cleaned description:\")\n",
    "print(df['clean_bd'].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize business description length before cleaning\n",
    "long_bd_length = df['long_bd'].dropna().apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(long_bd_length, bins=50, kde=True)\n",
    "plt.title('Business Description Length Before Cleaning (No. of Characters)')\n",
    "plt.xlabel('Length of BD')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize business description length after cleaning\n",
    "clean_bd_length = df['clean_bd'].dropna().apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(clean_bd_length, bins=50, kde=True, color='green')\n",
    "plt.title('Business Description Length After Cleaning (No. of Characters)')\n",
    "plt.xlabel('Length of Cleaned BD')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befd244",
   "metadata": {},
   "source": [
    "**Effect of Text Preprocessing**: By focusing on just the first two sentences, we significantly reduce the length of business descriptions while retaining their core essence. This approach helps filter out marketing language and focus on the fundamental operational descriptions of each company. The histogram shows how our preprocessing dramatically standardizes the text length, making it more suitable for embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c923de",
   "metadata": {},
   "source": [
    "## 4. SBERT Embedding Generation  \n",
    "\n",
    "In this section, I convert the processed business descriptions into numerical vectors (embeddings) using the Sentence-BERT (SBERT) model. SBERT is a modification of the BERT architecture optimized for generating semantically meaningful sentence embeddings.\n",
    "\n",
    "The model \"multi-qa-MiniLM-L6-cos-v1\" was chosen because it:\n",
    "- Is optimized for semantic similarity tasks\n",
    "- Has a good balance between model size and performance\n",
    "- Works well with short to medium-length texts (like our business descriptions)\n",
    "\n",
    "These embeddings capture the semantic meaning of the business descriptions, enabling us to measure similarity between companies based on the content of their descriptions rather than just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b379111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all business descriptions\n",
    "embeddings = model.encode(df['clean_bd'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Convert embeddings to numpy array for easier manipulation\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Print embedding shape to verify dimensions\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c02d13",
   "metadata": {},
   "source": [
    "## 5. Dimension Reduction\n",
    "\n",
    "The raw SBERT embeddings are high-dimensional (typically 384 dimensions), which can lead to the \"curse of dimensionality\" problem when clustering. To address this, we apply dimension reduction techniques in two stages:\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**: First, we use PCA to reduce the dimensionality while preserving as much variance as possible. This also helps us understand how many dimensions we actually need.\n",
    "\n",
    "2. **UMAP (Uniform Manifold Approximation and Projection)**: Then, we apply UMAP to further reduce dimensions while preserving both the local and global structure of the data. UMAP is particularly good at capturing non-linear relationships in the data.\n",
    "\n",
    "This two-step approach helps us create a more compact representation that's easier to visualize and cluster while maintaining the semantic relationships between companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77950ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA first to understand variance distribution\n",
    "pca = PCA(n_components=300)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot cumulative explained variance to help determine optimal number of components\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, 301), explained_variance, marker='o')\n",
    "plt.xlabel('Number of PCA components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Elbow Method for PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print cumulative variance at n = 100\n",
    "embedding_size = 100\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"Cumulative explained variance at 100 components: {cumulative_variance[embedding_size]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cbd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP for dimension reduction while preserving data structure\n",
    "umap_reducer = UMAP(\n",
    "    n_components=embedding_size,\n",
    "    random_state=42,\n",
    "    n_neighbors=15,  # Balance between local and global structure\n",
    "    min_dist=0.1     # Controls how tightly points are packed together\n",
    ")\n",
    "\n",
    "# Fit and transform the embeddings\n",
    "reduced_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "print(f\"Reduced embedding shape: {reduced_embeddings.shape}\")\n",
    "\n",
    "# Create a DataFrame with embeddings for further analysis\n",
    "embeddings_df = pd.DataFrame(\n",
    "    reduced_embeddings,\n",
    "    columns=[f'embedding_{i}' for i in range(reduced_embeddings.shape[1])]\n",
    ")\n",
    "\n",
    "# Join embeddings with company information\n",
    "df_with_embeddings = df.reset_index(drop=True).join(embeddings_df).sort_values(by=['industry', 'sub_industry'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf58677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for embedding plots\n",
    "def embedding_figure(ax, title: str = 'Title'):\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "# Create visualizations to check if embeddings preserve industry structure\n",
    "f, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot by industry (first two dimensions only)\n",
    "sns.scatterplot(\n",
    "    data=df_with_embeddings,\n",
    "    x='embedding_0', y='embedding_1',\n",
    "    hue='industry',\n",
    "    s=15, alpha=0.7,\n",
    "    ax=axs[0],\n",
    "    legend='brief'\n",
    ")\n",
    "axs[0].legend(title='Industry', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "embedding_figure(axs[0], 'All Industries')\n",
    "\n",
    "# Get top 20 sub-industries (for better visualization)\n",
    "top_20_subindustries = df_with_embeddings['sub_industry'].value_counts().nlargest(20).index.tolist()\n",
    "top_20_subindustry_data = df_with_embeddings[df_with_embeddings['sub_industry'].isin(top_20_subindustries)]\n",
    "\n",
    "# Plot by sub-industry (top 20 only)\n",
    "sns.scatterplot(\n",
    "    data=top_20_subindustry_data,\n",
    "    x='embedding_0', y='embedding_1',\n",
    "    hue='sub_industry',\n",
    "    s=15, alpha=0.7,\n",
    "    ax=axs[1],\n",
    "    legend='brief'\n",
    ")\n",
    "axs[1].legend(title='Sub-Industry', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "embedding_figure(axs[1], 'Top 20 Sub-Industries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a601fe",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is an ideal approach for this dataset because:\n",
    "\n",
    "1. It creates a hierarchical structure that can reveal relationships at different levels of granularity\n",
    "2. It produces a dendrogram visualization that makes it easy to interpret relationships\n",
    "\n",
    "I use Ward's linkage method, which minimizes the variance within each cluster. This tends to create more balanced, compact clusters compared to other linkage methods.\n",
    "\n",
    "The process involves:\n",
    "1. Standardizing the embedding features to give equal weight to all dimensions\n",
    "2. Computing a linkage matrix that defines the cluster hierarchy\n",
    "3. Creating a dendrogram to visualize the clustering structure\n",
    "4. Cutting the dendrogram at an appropriate level to create our final clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86d4b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize embeddings to give equal weight to all dimensions\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(reduced_embeddings)\n",
    "\n",
    "# Compute linkage matrix using Ward's method (minimizes variance within clusters)\n",
    "linkage_matrix = linkage(embeddings_scaled, method='ward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f76b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram to visualize hierarchical structure\n",
    "plt.figure(figsize=(12, 5))\n",
    "dendrogram(linkage_matrix, truncate_mode='level') \n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Company Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83996688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters by cutting the dendrogram to match the number of GICS sectors (11)\n",
    "cluster_labels = fcluster(linkage_matrix, t=11, criterion='maxclust')\n",
    "df_with_embeddings['cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ae6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine cluster size distribution\n",
    "cluster_counts = df_with_embeddings['cluster'].value_counts().sort_index()\n",
    "print(\"Number of companies in each cluster:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Industry distribution within each cluster (cross-tabulation)\n",
    "industry_cluster = pd.crosstab(df_with_embeddings['cluster'], df_with_embeddings['industry'])\n",
    "\n",
    "# Normalize by industry (columns) to see distribution of each industry across clusters\n",
    "industry_cluster_norm = industry_cluster.div(industry_cluster.sum(axis=0), axis=1)\n",
    "\n",
    "# Plot heatmap of industry distribution across clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(industry_cluster_norm, annot=True, fmt=\".2f\", cmap=\"YlOrBr\")\n",
    "plt.title(\"Cluster Distribution Within Each Industry (Normalized by Industry)\")\n",
    "plt.xlabel(\"Industry\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eff5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrograms for individual clusters to examine their internal structure\n",
    "for cluster in sorted(df_with_embeddings['cluster'].unique()):\n",
    "    # Filter data for the current cluster\n",
    "    cluster_data = df_with_embeddings[df_with_embeddings['cluster'] == cluster]\n",
    "    \n",
    "    # Skip very large clusters for clarity\n",
    "    if len(cluster_data) > 100:\n",
    "        print(f\"Cluster {cluster} has {len(cluster_data)} companies, skipping detailed dendrogram.\")\n",
    "        continue\n",
    "        \n",
    "    # Get indices of companies in this cluster\n",
    "    cluster_indices = cluster_data.index\n",
    "    \n",
    "    # Extract embeddings for this cluster\n",
    "    cluster_embeddings = embeddings_scaled[cluster_indices]\n",
    "    \n",
    "    # Compute linkage matrix for this cluster\n",
    "    if len(cluster_embeddings) > 1:  # Need at least 2 points for linkage\n",
    "        cluster_linkage = linkage(cluster_embeddings, method='ward')\n",
    "        \n",
    "        # Get company names for this cluster\n",
    "        company_names = cluster_data['company'].values\n",
    "        \n",
    "        # Plot dendrogram for this cluster\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        dendrogram(cluster_linkage, labels=company_names, leaf_rotation=90)\n",
    "        plt.title(f'Hierarchical Clustering Dendrogram for Cluster {cluster}')\n",
    "        plt.xlabel('Company')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Cluster {cluster} has only {len(cluster_embeddings)} company, skipping dendrogram.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337d808",
   "metadata": {},
   "source": [
    "## 7. Conclusion  \n",
    "\n",
    "This project successfully implemented a business description clustering pipeline using SBERT embeddings and hierarchical clustering. The approach revealed several interesting patterns in how S&P 500 companies can be grouped based on their semantic descriptions rather than traditional industry classifications.  \n",
    "\n",
    "### Key Findings  \n",
    "\n",
    "- **SBERT embeddings** effectively captured the semantic similarity between business descriptions, going beyond simple keyword matching  \n",
    "- The **dimensionality reduction pipeline** preserved meaningful structure while making the data suitable for clustering  \n",
    "- The **hierarchical clustering approach** revealed both expected sectoral groupings and unexpected cross-industry relationships  \n",
    "- Several companies were found to have more **operational similarity** with firms outside their traditional GICS sector than within it  \n",
    "\n",
    "### Implementation Value  \n",
    "\n",
    "The implementation demonstrates how natural language processing can complement traditional industry classification systems by providing insights into functional similarities between businesses. This approach could be valuable for:  \n",
    "\n",
    "- Portfolio diversification  \n",
    "- Competitive analysis  \n",
    "- Identifying companies with similar operational profiles across apparent industry boundaries  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
